{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ae785c-bf88-45f7-bb14-4fb3e16f6b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pylab as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.ticker as mtick\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import linregress\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "from spacepy import pycdf\n",
    "import warnings\n",
    "import moms_fast\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import pandas as pd\n",
    "\n",
    "import nnet_evaluate\n",
    "import utils\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f076dd9-e714-4750-a7f3-fcfa079c48fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf= h5py.File('/home/ubuntu/data/samples_train_n=50000_nosw.hdf')\n",
    "phases = list(hdf.keys())\n",
    "\n",
    "for phase in phases:\n",
    "    print(phase.ljust(20), hdf[phase]['E'].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa285419-3b38-4aa4-a925-1ffed7cd0856",
   "metadata": {},
   "source": [
    "# Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa6e0eb-7b88-4bae-862f-f52a8692e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    '/mnt/efs/dasilva/compression-cfha/data/nnet_models/hidden_layer_exp/4A_dusk_flank.rfr001/moments_stats.hdf',\n",
    "    '/mnt/efs/dasilva/compression-cfha/data/nnet_models/hidden_layer_exp/4B_dayside.rfr001/moments_stats.hdf',\n",
    "    '/mnt/efs/dasilva/compression-cfha/data/nnet_models/hidden_layer_exp/4C_dawn_flank.rfr001/moments_stats.hdf',\n",
    "    '/mnt/efs/dasilva/compression-cfha/data/nnet_models/hidden_layer_exp/4D_tail.rfr001/moments_stats.hdf',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4c30ca-55c8-46e8-8995-ea8cdb25ecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "moments = ['n']\n",
    "for file_name in files:\n",
    "    hdf = h5py.File(file_name, 'r')\n",
    "    sizes = hdf['sizes'][:]\n",
    "    r2 = {m: hdf[m]['r2'][:] for m in moments}\n",
    "\n",
    "    print(os.path.basename(file_name), r2['n'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff327804-102d-4352-81d5-e540547ca305",
   "metadata": {},
   "outputs": [],
   "source": [
    "moments = ['vx', 'vy', 'vz']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, sharex='all', sharey='all', figsize=(22, 6), dpi=300)\n",
    "\n",
    "for file_name in files:\n",
    "    hdf = h5py.File(file_name, 'r')\n",
    "    sizes = hdf['sizes'][:]\n",
    "    r2 = {m: hdf[m]['r2'][:] for m in moments}\n",
    "    points_true = {m: hdf[m]['points_true'][:] for m in moments}\n",
    "    points_recon = {m: hdf[m]['points_recon'][:] for m in moments}\n",
    "    hdf.close()\n",
    "    \n",
    "    region_title = 'Phase ' + os.path.basename(os.path.dirname(file_name)).replace(\"_\", \" \").split(\".\")[0]\n",
    "    region_title = ' '.join([word[0].upper() + word[1:] for word in region_title.split(' ')])\n",
    "\n",
    "    for i, m in enumerate(r2):\n",
    "        axes[i].plot(sizes/(32*16*2), r2[m], 'o-', label=region_title)\n",
    "        axes[i].set_title(m.capitalize(), fontsize=16)\n",
    "        axes[i].set_ylabel('Correlation Coefficient ($r^2$)', fontsize=16)\n",
    "        axes[i].set_xlabel('Dimensionality Reduction (Fraction)', fontsize=16)\n",
    "\n",
    "    axes[i].set_ylim(0.75, 1)\n",
    "for ax in axes:\n",
    "    #ax.axhline(1, color='black', linestyle='dashed')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "axes[-1].legend(ncol=1, bbox_to_anchor = (1.55, .85), loc='center right', fontsize=14)\n",
    "    \n",
    "    \n",
    "fig.suptitle('Neural Network Bulk Velocity Reconstruction Correlation For Each Orbital Configuration', fontweight='bold', fontsize=20)\n",
    "fig.tight_layout()\n",
    "fig.savefig('Figure2.jpg')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eb6336-d175-44a3-a76f-66173cae87bb",
   "metadata": {},
   "source": [
    "# Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4817c41-5238-4a45-91c0-835954cde5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(cdf_filename):\n",
    "\n",
    "    N_EN = 32\n",
    "    N_EN_SHELLS = 2\n",
    "    N_PHI = 32\n",
    "    N_THETA = 16\n",
    "    cdf = pycdf.CDF(cdf_filename)\n",
    "\n",
    "    dist = cdf['mms1_dis_dist_brst'][:]\n",
    "    dist_err = cdf['mms1_dis_disterr_brst'][:]\n",
    "    epoch = cdf['Epoch'][:]\n",
    "    ntime = epoch.size\n",
    "    counts = np.zeros((ntime, N_PHI, N_THETA, N_PHI))\n",
    "\n",
    "    for i in range(ntime):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore')\n",
    "            tmp_counts = np.square(dist[i] / dist_err[i])\n",
    "        tmp_counts[np.isnan(tmp_counts)] = 0\n",
    "        tmp_counts = np.rint(tmp_counts)\n",
    "        counts[i] = tmp_counts\n",
    "\n",
    "    cdf.close()\n",
    "\n",
    "    return epoch, counts, ntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a22b87f-7bd9-4c64-9b06-ff68b528682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = nnet_evaluate.load_test_data('4B_dayside')\n",
    "f1ct = utils.get_f1ct({'4B_dayside': test_data}, ['4B_dayside'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28b66be-3c2b-427e-966c-1efcc13ab170",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = np.array([2.160000e+00, 3.910000e+00, 7.070000e+00, 1.093000e+01,\n",
    "       1.424000e+01, 1.854000e+01, 2.414000e+01, 3.144000e+01,\n",
    "       4.094000e+01, 5.332000e+01, 6.944000e+01, 9.043000e+01,\n",
    "       1.177700e+02, 1.533600e+02, 1.997200e+02, 2.601000e+02,\n",
    "       3.387200e+02, 4.411100e+02, 5.744500e+02, 7.481000e+02,\n",
    "       9.742300e+02, 1.268720e+03, 1.652240e+03, 2.151680e+03,\n",
    "       2.802100e+03, 3.649120e+03, 4.752190e+03, 6.188690e+03,\n",
    "       8.059430e+03, 1.049565e+04, 1.366831e+04, 1.780000e+04],\n",
    "      dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d042e5-4927-450a-9b38-667a0a10e287",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EN = 32\n",
    "N_EN_SHELLS = 2\n",
    "N_PHI = 32\n",
    "N_THETA = 16\n",
    "\n",
    "#hdf_filename = glob.glob('/home/ubuntu/data/recons/4B_dayside-100/*.hdf5')[95]\n",
    "hdf_filename = '../compression-cfha_code/example.h5'\n",
    "print(hdf_filename)\n",
    "#cdf_filename = '/mnt/efs/dasilva/compression-cfha/data/mms_data/4B_dayside/' + os.path.basename(hdf_filename).replace('.hdf5', '.cdf')\n",
    "cdf_filename = '../compression-cfha_code/mms1_example.cdf'\n",
    "gpc_filename = hdf_filename.replace('.h5', '.gpc')\n",
    "gpc_filename = gpc_filename.replace('.hdf5', '.gpc')\n",
    "\n",
    "print(gpc_filename)\n",
    "epoch, counts, ntime = get_data(cdf_filename)\n",
    "\n",
    "hdf = h5py.File(hdf_filename, 'r')\n",
    "counts_recon = hdf['counts'][:]\n",
    "hdf.close()\n",
    "\n",
    "moms_true = [moms_fast.fast_moments(f1ct * c) for c in counts]\n",
    "moms_recon = [moms_fast.fast_moments(f1ct * c) for c in counts_recon]\n",
    "\n",
    "\n",
    "cmpr_ratio =  (32 * 16 * 32 * 16 * epoch.size) / (os.path.getsize(gpc_filename) * 8)\n",
    "file_size = os.path.getsize(gpc_filename) / epoch.size\n",
    "vars = ['Strue', 'Srecon', 'Srel']\n",
    "fig, axes = plt.subplots(len(vars), 1, figsize=(15, 4*len(vars)), dpi=300, sharex=True)\n",
    "\n",
    "\n",
    "Escale = np.zeros((32, counts.shape[0]))\n",
    "for k in range(Escale.shape[1]):\n",
    "    Escale[:, k] = E\n",
    "\n",
    "caxes = []\n",
    "\n",
    "\n",
    "a = (counts).mean(axis=((1, 2))).T\n",
    "b = (counts_recon).mean(axis=((1, 2))).T\n",
    "rel_error = 100 * (a - b)/a\n",
    "    \n",
    "for i, var in enumerate(vars):\n",
    "    vmin = .01\n",
    "    vmax = 15\n",
    "    if var == 'Strue':\n",
    "        im = axes[i].pcolor(epoch, E, (counts).mean(axis=((1, 2))).T, norm=LogNorm(vmin=vmin, vmax=vmax), cmap='plasma')\n",
    "        axes[i].set_ylabel('$\\\\bf{Original~Ion~Data}$\\nEnergy [eV]', fontsize=18)\n",
    "        \n",
    "        divider = make_axes_locatable(axes[i])\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "        fig.colorbar(im, cax=cax, orientation='vertical').set_label('Average Counts\\nper Energy Channel', fontsize=16)\n",
    "    elif var == 'Srecon':\n",
    "        im = axes[i].pcolor(epoch, E, (counts_recon).mean(axis=((1, 2))).T, norm=LogNorm(vmin=vmin, vmax=vmax), cmap='plasma')\n",
    "        axes[i].set_ylabel('$\\\\bf{Compressed~Ion~Data}$\\nEnergy [eV]', fontsize=18)\n",
    "        \n",
    "        divider = make_axes_locatable(axes[i])\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "        fig.colorbar(im, cax=cax, orientation='vertical').set_label('Average Counts\\nper Energy Channel', fontsize=16)\n",
    "    elif var == 'Srel':\n",
    "        im = axes[i].pcolor(epoch, E, rel_error, vmin=-.25, vmax=.25, cmap='bwr')\n",
    "        axes[i].set_ylabel('$\\\\bf{Relative~Error}$\\nEnergy [eV]', fontsize=18)\n",
    "        #axes[i].set_yscale('log')\n",
    "        divider = make_axes_locatable(axes[i])\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "        fig.colorbar(im, cax=cax, orientation='vertical').set_label('%', fontsize=16)\n",
    "\n",
    "    axes[i].set_yscale('log')\n",
    "    caxes.append(cax)\n",
    "\n",
    "axes[0].set_title(f'Demonstration of Compression Spectrogram (Compression Ratio: {cmpr_ratio:.1f}X)', fontweight='bold', fontsize=20)\n",
    "fig.tight_layout()\n",
    "\n",
    "for ax in axes.tolist() + caxes:\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "fig.savefig('Figure3.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c44137e-e4ac-4ba5-b82e-8723fd7d89bd",
   "metadata": {},
   "source": [
    "# Figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d05b5ef-5eed-4b56-a9eb-4b9e843ca31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = ['n', 'vx', 'vy', 'vz', 'txx', 'tyy', 'tzz']\n",
    "fig, axes = plt.subplots(len(vars), 1, figsize=(15, 2.5*len(vars)), dpi=300, sharex=True)\n",
    "\n",
    "for i, var in enumerate(vars):\n",
    "\n",
    "    #axes[i].set_title(var, fontsize=16)\n",
    "    axes[i].plot(epoch, [d[var] for d in moms_true], label=f'True')\n",
    "    axes[i].plot(epoch, [d[var] for d in moms_recon], label=f'Reconstructed')\n",
    "        \n",
    "    if i == 0:\n",
    "        axes[i].legend(ncol=2, fontsize=18)\n",
    "    if var == 'n':\n",
    "        axes[i].set_ylim([0, 1.1 * np.max([d[var] for d in moms_true])])\n",
    "        axes[i].set_ylabel('n ($cm^{-3}$)', fontsize=16)\n",
    "    elif var[0] == 'v':\n",
    "        axes[i].set_ylabel(f'{var.capitalize()} (km/s)', fontsize=16)\n",
    "    elif var[0] == 't':\n",
    "        axes[i].set_ylabel(f'{var.capitalize()} (eV)', fontsize=16)\n",
    "        axes[i].set_ylim(0, 600)\n",
    "    axes[i].set_xlim(epoch[0], epoch[-1])\n",
    "\n",
    "    \n",
    "axes[0].set_title(f'Demonstration of Compression Moments (Compression Ratio: {cmpr_ratio:.1f}X)', fontweight='bold', fontsize=20)\n",
    "fig.tight_layout()\n",
    "\n",
    "for ax in axes.tolist():\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "axes[0].text(epoch[1420], 5, 'Density\\nOscillations', fontsize=16)\n",
    "axes[2].text(epoch[900], -130, 'Vy Direction\\nSwitch', fontsize=16)\n",
    "\n",
    "#axes[1].text(epoch[220], -350, 'Transition\\nRegion', fontsize=16)\n",
    "axes[4].text(epoch[220], 50, 'Transition\\nRegion', fontsize=16)\n",
    "axes[5].text(epoch[220], 50, 'Transition\\nRegion', fontsize=16)\n",
    "axes[6].text(epoch[220], 50, 'Transition\\nRegion', fontsize=16)\n",
    "\n",
    "axes[4].text(epoch[1400], 200, 'Transition\\nRegion', fontsize=16)\n",
    "axes[5].text(epoch[1400], 200, 'Transition\\nRegion', fontsize=16)\n",
    "axes[6].text(epoch[1400], 200, 'Transition\\nRegion', fontsize=16)\n",
    "\n",
    "    \n",
    "#os.makedirs('plots', exist_ok=True)\n",
    "fig.subplots_adjust(hspace=.1)\n",
    "fig.savefig('Figure4.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ea4f91-059f-46fe-95a8-050bd7982229",
   "metadata": {},
   "source": [
    "# Figure 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d80e65-4ed7-4292-940a-8ad3175061be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py, pandas as pd\n",
    "\n",
    "def get_row(phase, file_name):\n",
    "    hdf = h5py.File(file_name, 'r')\n",
    "    i = np.argmin(np.abs(hdf['sizes'][:] - 100))\n",
    "\n",
    "\n",
    "    row = {'phase': phase}\n",
    "\n",
    "    for var in ['n', 'vx', 'vy', 'vz', 'txx', 'tyy', 'tzz']:\n",
    "        points_recon = hdf[var]['points_recon'][i, :]\n",
    "        points_true = hdf[var]['points_true'][i, :]\n",
    "        err = (points_recon - points_true) \n",
    "        \n",
    "        ls = sorted(err)\n",
    "        \n",
    "        alpha = 0.05\n",
    "        low = ls[int(alpha/2 * len(ls))]\n",
    "        hi = ls[int((1-alpha/2) * len(ls))]\n",
    "        \n",
    "        row[f'{var}_mean'] = np.mean(err)\n",
    "        row[f'{var}_std'] = np.std(err)\n",
    "        row[f'{var}_median'] = np.median(err)\n",
    "        row[f'{var}_ci_low'] = low\n",
    "        row[f'{var}_ci_hi'] = hi\n",
    "        \n",
    "    return row\n",
    "\n",
    "    \n",
    "df_rows = [\n",
    "    get_row('4A Dusk Flank', '/mnt/efs/dasilva/compression-cfha/data/nnet_models/hidden_layer_exp/4A_dusk_flank.rfr001/moments_stats.hdf'),\n",
    "    get_row('4B Dayside', '/mnt/efs/dasilva/compression-cfha/data/nnet_models/hidden_layer_exp/4B_dayside.rfr001/moments_stats.hdf'),\n",
    "    get_row('4C Dawn Flank', '/mnt/efs/dasilva/compression-cfha/data/nnet_models/hidden_layer_exp/4C_dawn_flank.rfr001/moments_stats.hdf'),\n",
    "    get_row('4D Tail', '/mnt/efs/dasilva/compression-cfha/data/nnet_models/hidden_layer_exp/4D_tail.rfr001/moments_stats.hdf')\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(df_rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9705209-25d0-4a6b-aa7d-93f5e1c75b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = ['_', 'n', '_', 'vx', 'vy', 'vz', 'txx', 'tyy', 'tzz']\n",
    "\n",
    "fig, axes = plt.subplots(3, len(vars)//3, sharex='all', figsize=(18, 8), dpi=300)\n",
    "axes_orig = axes\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, var in enumerate(vars):\n",
    "    \n",
    "    if var == '_':\n",
    "        axes[i].set_axis_off()\n",
    "        continue\n",
    "        \n",
    "    color = {'n': 'blue', 'vx': 'gold', 'vy': 'gold', 'vz': 'gold', 'txx': 'red', 'tyy': 'red', 'tzz': 'red'}[var]\n",
    "        \n",
    "    axes[i].errorbar(x=df['phase'], y=df[f'{var}_median'], yerr=(df[f'{var}_median'] - df[f'{var}_ci_low'], df[f'{var}_ci_hi'] - df[f'{var}_median']), color=color, ecolor='k', capsize=3)\n",
    "    axes[i].plot(df['phase'], df[f'{var}_median'], 'ko')\n",
    "    if var == 'n':\n",
    "        axes[i].set_ylabel('n Error ($cm^{-3}$)', fontsize=16)\n",
    "        #axes[i].set_ylim([-12, 12])\n",
    "    elif var[0] == 'v':\n",
    "        axes[i].set_ylabel(f'{var.capitalize()} Error (km/s)', fontsize=16)\n",
    "        #axes[i].set_ylim([-160, 160])\n",
    "    elif var[0] == 't':\n",
    "        axes[i].set_ylabel(f'{var.capitalize()} Error (eV)', fontsize=16)\n",
    "        #axes[i].set_ylim(-500, 500)\n",
    "for ax in axes.tolist():\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "    ax.set_xticklabels([s.split(' ', 1)[0] + '\\n' + s.split(' ', 1)[1] for s in df.phase])\n",
    "\n",
    "fig.suptitle(f'95% Confidence Intervals of Signed Moments Error (Compression Ratio: {cmpr_ratio:.1f}X)', fontweight='bold', fontsize=20)\n",
    "fig.tight_layout()\n",
    "fig.savefig('Figure5.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cd5c51-73c7-45b7-8177-16ec26eb4c4f",
   "metadata": {},
   "source": [
    "# Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af634d2-3e64-4228-a8e1-f882262a5024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py, pandas as pd\n",
    "\n",
    "def get_row(phase, file_name):\n",
    "    hdf = h5py.File(file_name, 'r')\n",
    "    i = np.argmin(np.abs(hdf['sizes'][:] - 100))\n",
    "\n",
    "    row = {'phase': phase}\n",
    "\n",
    "    for var in ['n', 'vx', 'vy', 'vz', 'txx', 'tyy', 'tzz']:\n",
    "        points_true = hdf[var]['points_true'][i, :]\n",
    "                \n",
    "        ls = sorted(points_true)\n",
    "        \n",
    "        alpha = 0.05\n",
    "        low = ls[int(alpha/2 * len(ls))]\n",
    "        hi = ls[int((1-alpha/2) * len(ls))]\n",
    "\n",
    "        row[var] = '%.3f - %.3f' % (low, hi)\n",
    "    return row\n",
    "\n",
    "    \n",
    "df_rows = [\n",
    "    get_row('4A Dusk Flank', '/mnt/efs/dasilva/compression-cfha/data/nnet_models/hidden_layer_exp/4A_dusk_flank.rfr001/moments_stats.hdf'),\n",
    "    get_row('4B Dayside', '/mnt/efs/dasilva/compression-cfha/data/nnet_models/hidden_layer_exp/4B_dayside.rfr001/moments_stats.hdf'),\n",
    "    get_row('4C Dawn Flank', '/mnt/efs/dasilva/compression-cfha/data/nnet_models/hidden_layer_exp/4C_dawn_flank.rfr001/moments_stats.hdf'),\n",
    "    get_row('4D Tail', '/mnt/efs/dasilva/compression-cfha/data/nnet_models/hidden_layer_exp/4D_tail.rfr001/moments_stats.hdf')\n",
    "]\n",
    "\n",
    "pd.DataFrame(df_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff4ec04-fee1-4aa1-984c-56a06274924c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651cdec8-4e47-4242-9986-a47af22d1b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdb7543-db5d-430a-b799-c795b0b2a2ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Compression-CFHA",
   "language": "python",
   "name": "compression-cfha"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
